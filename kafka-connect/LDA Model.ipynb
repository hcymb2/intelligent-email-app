{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e4baf98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05dd964e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "167efce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "npr = pd.read_csv('npr1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "119f3182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Older women who look on the bright side of lif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In Bangladesh, a new report finds, impoverishe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When he first moved to Miami, Waltter Teruel s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When ATT, a leading Internet provider, propose...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Donald Trump is on a tour of battleground st...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article\n",
       "0  Older women who look on the bright side of lif...\n",
       "1  In Bangladesh, a new report finds, impoverishe...\n",
       "2  When he first moved to Miami, Waltter Teruel s...\n",
       "3  When ATT, a leading Internet provider, propose...\n",
       "4    Donald Trump is on a tour of battleground st..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fff9e55a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "509"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to list\n",
    "data = npr.values.tolist()\n",
    "len(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2625211f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18e59f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "509"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c549ea6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea828694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "nlp = spacy.load(\"en_core_web_sm\",disable=['parser']) \n",
    "\n",
    "##Adding additional stopwords\n",
    "new_stopwords= ['dear', 'thanks','regards', 'hello','hi', 'bye','goodbye', 'say']\n",
    "for word in new_stopwords:\n",
    "    nlp.Defaults.stop_words.add(word)\n",
    "    nlp.vocab[word].is_stop = True\n",
    "    \n",
    "def remove_stopwords_spacy(texts):\n",
    "    return [[word.text for word in nlp(str(text)) if not word.is_stop] for text in texts]\n",
    "    #return [[word.orth_ for word in nlp(str(text)) if word not in nlp.Defaults.stop_words] for text in texts]\n",
    "\n",
    "def remove_stopwords_gensim(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18cf5a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "\n",
    "data_words_nostops = remove_stopwords_spacy(data_words)\n",
    "#data_words_nostops = remove_stopwords_gensim(data_words_nostops)\n",
    "\n",
    "#remove words that only occur once to make process faster\n",
    "all_tokens = sum(data_words_nostops, [])\n",
    "tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)\n",
    "text_no_single_words = [[term for term in words if term not in tokens_once] for words in data_words_nostops]\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "493a31df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus \n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f5a75fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the number of topics for LDA using HDP\n",
    "#hdp_model = gensim.models.hdpmodel.HdpModel(corpus=corpus, id2word=id2word)\n",
    "#hdp_topics= hdp_model.print_topics()\n",
    "#for topic in hdp_topics:\n",
    "    #print(topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08349fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score [0.2544804347153319, 0.28176149636514136, 0.3014145047721189, 0.3502119920451648, 0.3874874672026672, 0.3627511320034637, 0.38055339667098886, 0.37687023913139384, 0.4103855876351199, 0.42085411453099136, 0.3962621878827782, 0.4272256673251504, 0.49276949372165835, 0.4391421070664864, 0.4507258092221508, 0.45156901431903307, 0.4521610378758477, 0.4584206342440234, 0.4294496576449215, 0.4324650168580141]\n",
      "Perplexity Score [-8.477639622212628, -8.419906712834612, -8.405121608313172, -8.391348118555813, -8.38818905923543, -8.39845366588185, -8.403582022998092, -8.45347183295547, -8.59350276489136, -8.787612238639518, -9.15746067527672, -9.4857831873928, -9.895573895662746, -10.214646969235881, -10.385783259104166, -10.54412929596745, -10.71462136692859, -10.868532369018276, -10.998548166159186, -11.158216355984814]\n"
     ]
    }
   ],
   "source": [
    "# Build LDA model\n",
    "coherence_score=[]\n",
    "perplexity_score=[]\n",
    "for i in range(1,21):\n",
    "    \n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                               id2word=id2word,\n",
    "                                               num_topics=i, \n",
    "                                               random_state=110,\n",
    "                                               update_every=1,\n",
    "                                               chunksize=100,\n",
    "                                               passes=10,\n",
    "                                               alpha='auto',\n",
    "                                               per_word_topics=False,\n",
    "                                               minimum_probability=0.0)\n",
    "\n",
    "    perplexity_score.append(lda_model.log_perplexity(corpus))\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    coherence_score.append(coherence_model_lda.get_coherence())\n",
    "\n",
    "print('Coherence Score', coherence_score)\n",
    "\n",
    "print('Perplexity Score',perplexity_score)\n",
    "\n",
    "#from pprint import pprint\n",
    "#pprint(lda_model.print_topics())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5baf7a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = lambda i: coherence_score[i]\n",
    "\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                               id2word=id2word,\n",
    "                                               num_topics=max(range(len(coherence_score)), key=f)+1, \n",
    "                                               random_state=110,\n",
    "                                               update_every=1,\n",
    "                                               chunksize=100,\n",
    "                                               passes=10,\n",
    "                                               alpha='auto',\n",
    "                                               per_word_topics=False,\n",
    "                                               minimum_probability=0.0)\n",
    "\n",
    "\n",
    "max(range(len(coherence_score)), key=f)+1\n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dbe79d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.13778025), (1, 0.0001236082), (2, 0.18513234), (3, 0.09968137), (4, 0.0015244875), (5, 0.0005962362), (6, 0.0005161454), (7, 3.2855874e-05), (8, 0.0002037737), (9, 0.00019994486), (10, 0.00023227147), (11, 0.0002154149), (12, 0.57376134)]\n"
     ]
    }
   ],
   "source": [
    "#putting into dataframe the assinged topic of each document/email\n",
    "from itertools import chain\n",
    "lda_corpus = lda_model[corpus]\n",
    "\n",
    "all_topics=lda_model.get_document_topics(corpus)\n",
    "print(all_topics[0])\n",
    "\n",
    "scores = list(chain(*[[score for topic_id,score in topic] \\\n",
    "                      for topic in [doc for doc in lda_corpus]]))\n",
    "threshold = sum(scores)/len(scores)\n",
    "\n",
    "\n",
    "all_topics_csr = gensim.matutils.corpus2csc(all_topics)\n",
    "all_topics_numpy = all_topics_csr.T.toarray()\n",
    "categorized_docs=[]\n",
    "\n",
    "for document in all_topics_numpy:\n",
    "    if max(document) > threshold:\n",
    "        categorized_docs.append(document.argmax())\n",
    "    else:\n",
    "        categorized_docs.append('Unmarked')\n",
    "\n",
    "npr = npr.assign(Topic = categorized_docs)\n",
    " \n",
    "#npr['Topic'] = all_topics_numpy.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "344e9670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Article  Topic  Category\n",
      "0  Older women who look on the bright side of lif...     12    Health\n",
      "1  In Bangladesh, a new report finds, impoverishe...      3      Work\n",
      "2  When he first moved to Miami, Waltter Teruel s...     12      Work\n",
      "3  When ATT, a leading Internet provider, propose...      3      Work\n",
      "4    Donald Trump is on a tour of battleground st...      3  Politics\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3     252\n",
       "12    122\n",
       "0      36\n",
       "5      34\n",
       "4      32\n",
       "11     10\n",
       "2       8\n",
       "8       7\n",
       "6       6\n",
       "10      2\n",
       "Name: Topic, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(npr.head())\n",
    "npr['Topic'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef6b74ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['say',\n",
       "  'police',\n",
       "  'officer',\n",
       "  'case',\n",
       "  'report',\n",
       "  'china',\n",
       "  'year',\n",
       "  'law',\n",
       "  'kill',\n",
       "  'death',\n",
       "  'number',\n",
       "  'official',\n",
       "  'enforcement',\n",
       "  'authority',\n",
       "  'attack',\n",
       "  'court',\n",
       "  'government',\n",
       "  'russia',\n",
       "  'incident',\n",
       "  'chinese'],\n",
       " 1: ['courage',\n",
       "  'grief',\n",
       "  'pulse',\n",
       "  'countless',\n",
       "  'wit',\n",
       "  'stump',\n",
       "  'brandon',\n",
       "  'glenn',\n",
       "  'jet',\n",
       "  'equality',\n",
       "  'dance',\n",
       "  'atlantic',\n",
       "  'golden',\n",
       "  'pearl',\n",
       "  'dancer',\n",
       "  'production',\n",
       "  'float',\n",
       "  'feb',\n",
       "  'invitation',\n",
       "  'legacy'],\n",
       " 2: ['health',\n",
       "  'child',\n",
       "  'abortion',\n",
       "  'zika',\n",
       "  'woman',\n",
       "  'service',\n",
       "  'percent',\n",
       "  'mojica',\n",
       "  'patient',\n",
       "  'program',\n",
       "  'virus',\n",
       "  'rico',\n",
       "  'department',\n",
       "  'rate',\n",
       "  'care',\n",
       "  'medical',\n",
       "  'hospital',\n",
       "  'plan',\n",
       "  'disability',\n",
       "  'special'],\n",
       " 3: ['say',\n",
       "  'time',\n",
       "  'year',\n",
       "  'new',\n",
       "  'tell',\n",
       "  'know',\n",
       "  'day',\n",
       "  'want',\n",
       "  'go',\n",
       "  'think',\n",
       "  'people',\n",
       "  'work',\n",
       "  'come',\n",
       "  'trump',\n",
       "  'way',\n",
       "  'write',\n",
       "  'president',\n",
       "  'word',\n",
       "  'thing',\n",
       "  'address'],\n",
       " 4: ['say',\n",
       "  'year',\n",
       "  'people',\n",
       "  'panda',\n",
       "  'community',\n",
       "  'resident',\n",
       "  'pan',\n",
       "  'home',\n",
       "  'family',\n",
       "  'city',\n",
       "  'local',\n",
       "  'muslim',\n",
       "  'story',\n",
       "  'day',\n",
       "  'church',\n",
       "  'work',\n",
       "  'car',\n",
       "  'son',\n",
       "  'know',\n",
       "  'conservation'],\n",
       " 5: ['trump',\n",
       "  'election',\n",
       "  'vote',\n",
       "  'clinton',\n",
       "  'brexit',\n",
       "  'say',\n",
       "  'republican',\n",
       "  'president',\n",
       "  'obama',\n",
       "  'state',\n",
       "  'party',\n",
       "  'nissan',\n",
       "  'law',\n",
       "  'white',\n",
       "  'voter',\n",
       "  'people',\n",
       "  'democratic',\n",
       "  'sunderland',\n",
       "  'senate',\n",
       "  'house'],\n",
       " 6: ['water',\n",
       "  'christma',\n",
       "  'santa',\n",
       "  'earth',\n",
       "  'planet',\n",
       "  'year',\n",
       "  'holiday',\n",
       "  'drink',\n",
       "  'space',\n",
       "  'charlie',\n",
       "  'city',\n",
       "  'bunch',\n",
       "  'nickname',\n",
       "  'tree',\n",
       "  'smithsonian',\n",
       "  'joaquin',\n",
       "  'god',\n",
       "  'patch',\n",
       "  'pipeline',\n",
       "  'emerita'],\n",
       " 7: ['monkey',\n",
       "  'cortex',\n",
       "  'fitch',\n",
       "  'ape',\n",
       "  'vowel',\n",
       "  'nonhuman',\n",
       "  'ishiyama',\n",
       "  'somatosensory',\n",
       "  'tickled',\n",
       "  'larynx',\n",
       "  'distinguishable',\n",
       "  'vienna',\n",
       "  'macaque',\n",
       "  'scholarly',\n",
       "  'tecumseh',\n",
       "  'vastly',\n",
       "  'caesar',\n",
       "  'yawn',\n",
       "  'emiliano',\n",
       "  'intelligible']}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##After getting making the LDA model now we will take top twenty words from each topic.\n",
    "##We will further also make a list containing top twenty words for each topic (its a list of lists)\n",
    "\n",
    "topics_with_words=[] #list that contains words belonging to each topic\n",
    "topic_num_list=[] #topic numbers\n",
    "for i in range(topic_num):\n",
    "    tt = lda_model.get_topic_terms(i,20)\n",
    "    topics_with_words.append([id2word[pair[0]] for pair in tt]) #filling list with each topics word\n",
    "    topic_num_list.append(i) #filling up the topic number list   \n",
    "    \n",
    "#removing stopwords again and removing words that are not in the vocab     \n",
    "for topic in topics_with_words:\n",
    "    topic= [word for word in topic if not nlp.vocab.has_vector(word)]\n",
    "    #cleanText = \" \".join([token.text for token in tokens if token.has_vector]) \n",
    "    topic= [word for word in topic if not nlp.vocab[word].is_stop]       \n",
    "topic_dict = dict(zip(topic_num_list, topics_with_words))#buliding a dictionary with list of topic numbers and list of words in those topics\n",
    "topic_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4071163d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that takes in topic words and a category and returns similary between the two\n",
    "#topic is a list and category should be passsed in as a spacy token\n",
    "def calculate_similarity(topic_words, category):\n",
    "    total_sim=0\n",
    "    string=''\n",
    "    #print(\"This is the string when we enter the function: {}\".format(string))\n",
    "    string = ' '.join([str(elem) for elem in topic_words])## this is the string that contains every word in that topic\n",
    "    #print(\"This is the string when we fill it up: {}\".format(string))\n",
    "    doc= spacy_model(string) ##topic's string is used created a spacy doc\n",
    "    for token in doc:\n",
    "        total_sim+= token.similarity(category)\n",
    "    return total_sim    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1dd5df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.vocab.has_vector(\"epa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "15ccd64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-a5caf7cacadc>:11: UserWarning: [W008] Evaluating Token.similarity based on empty vectors.\n",
      "  total_sim+= token.similarity(category)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['say', 'police', 'officer', 'case', 'report', 'china', 'year', 'law', 'kill', 'death', 'number', 'official', 'enforcement', 'authority', 'attack', 'court', 'government', 'russia', 'incident', 'chinese']\n",
      "\n",
      "\n",
      "['courage', 'grief', 'pulse', 'countless', 'wit', 'stump', 'brandon', 'glenn', 'jet', 'equality', 'dance', 'atlantic', 'golden', 'pearl', 'dancer', 'production', 'float', 'feb', 'invitation', 'legacy']\n",
      "\n",
      "\n",
      "['health', 'child', 'abortion', 'zika', 'woman', 'service', 'percent', 'mojica', 'patient', 'program', 'virus', 'rico', 'department', 'rate', 'care', 'medical', 'hospital', 'plan', 'disability', 'special']\n",
      "\n",
      "\n",
      "['say', 'time', 'year', 'new', 'tell', 'know', 'day', 'want', 'go', 'think', 'people', 'work', 'come', 'trump', 'way', 'write', 'president', 'word', 'thing', 'address']\n",
      "\n",
      "\n",
      "['say', 'year', 'people', 'panda', 'community', 'resident', 'pan', 'home', 'family', 'city', 'local', 'muslim', 'story', 'day', 'church', 'work', 'car', 'son', 'know', 'conservation']\n",
      "\n",
      "\n",
      "['trump', 'election', 'vote', 'clinton', 'brexit', 'say', 'republican', 'president', 'obama', 'state', 'party', 'nissan', 'law', 'white', 'voter', 'people', 'democratic', 'sunderland', 'senate', 'house']\n",
      "\n",
      "\n",
      "['water', 'christma', 'santa', 'earth', 'planet', 'year', 'holiday', 'drink', 'space', 'charlie', 'city', 'bunch', 'nickname', 'tree', 'smithsonian', 'joaquin', 'god', 'patch', 'pipeline', 'emerita']\n",
      "\n",
      "\n",
      "['monkey', 'cortex', 'fitch', 'ape', 'vowel', 'nonhuman', 'ishiyama', 'somatosensory', 'tickled', 'larynx', 'distinguishable', 'vienna', 'macaque', 'scholarly', 'tecumseh', 'vastly', 'caesar', 'yawn', 'emiliano', 'intelligible']\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Personal</th>\n",
       "      <th>Work</th>\n",
       "      <th>Studies</th>\n",
       "      <th>Meeting</th>\n",
       "      <th>School</th>\n",
       "      <th>Bussiness</th>\n",
       "      <th>News</th>\n",
       "      <th>Subscriptions</th>\n",
       "      <th>Marketing</th>\n",
       "      <th>Clients</th>\n",
       "      <th>...</th>\n",
       "      <th>Economy</th>\n",
       "      <th>Finance</th>\n",
       "      <th>Music</th>\n",
       "      <th>Family</th>\n",
       "      <th>College</th>\n",
       "      <th>Bills</th>\n",
       "      <th>Games</th>\n",
       "      <th>Design</th>\n",
       "      <th>Advertisements</th>\n",
       "      <th>Reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.922018</td>\n",
       "      <td>5.867961</td>\n",
       "      <td>5.000954</td>\n",
       "      <td>5.490351</td>\n",
       "      <td>6.148301</td>\n",
       "      <td>5.441613</td>\n",
       "      <td>5.914790</td>\n",
       "      <td>1.879558</td>\n",
       "      <td>3.822869</td>\n",
       "      <td>4.235974</td>\n",
       "      <td>...</td>\n",
       "      <td>5.023850</td>\n",
       "      <td>5.183203</td>\n",
       "      <td>3.779370</td>\n",
       "      <td>5.361845</td>\n",
       "      <td>5.124218</td>\n",
       "      <td>5.183096</td>\n",
       "      <td>3.625114</td>\n",
       "      <td>3.316906</td>\n",
       "      <td>2.809899</td>\n",
       "      <td>2.550122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.832694</td>\n",
       "      <td>3.845042</td>\n",
       "      <td>2.612650</td>\n",
       "      <td>2.576775</td>\n",
       "      <td>3.720851</td>\n",
       "      <td>3.291850</td>\n",
       "      <td>2.880136</td>\n",
       "      <td>1.034950</td>\n",
       "      <td>2.567267</td>\n",
       "      <td>2.384122</td>\n",
       "      <td>...</td>\n",
       "      <td>3.213712</td>\n",
       "      <td>2.733543</td>\n",
       "      <td>4.429894</td>\n",
       "      <td>3.707040</td>\n",
       "      <td>3.608046</td>\n",
       "      <td>2.913475</td>\n",
       "      <td>2.674548</td>\n",
       "      <td>3.219880</td>\n",
       "      <td>1.654688</td>\n",
       "      <td>1.225528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.069281</td>\n",
       "      <td>5.746724</td>\n",
       "      <td>5.241623</td>\n",
       "      <td>4.503539</td>\n",
       "      <td>6.150415</td>\n",
       "      <td>5.351885</td>\n",
       "      <td>4.313432</td>\n",
       "      <td>2.876343</td>\n",
       "      <td>4.056960</td>\n",
       "      <td>4.942735</td>\n",
       "      <td>...</td>\n",
       "      <td>4.210724</td>\n",
       "      <td>4.839167</td>\n",
       "      <td>3.281057</td>\n",
       "      <td>5.602248</td>\n",
       "      <td>5.394762</td>\n",
       "      <td>5.273412</td>\n",
       "      <td>2.706662</td>\n",
       "      <td>3.450992</td>\n",
       "      <td>2.888514</td>\n",
       "      <td>2.491245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.604465</td>\n",
       "      <td>9.902568</td>\n",
       "      <td>5.151183</td>\n",
       "      <td>6.706818</td>\n",
       "      <td>7.189224</td>\n",
       "      <td>7.277782</td>\n",
       "      <td>6.256900</td>\n",
       "      <td>2.989461</td>\n",
       "      <td>5.743333</td>\n",
       "      <td>6.160741</td>\n",
       "      <td>...</td>\n",
       "      <td>6.047608</td>\n",
       "      <td>5.063769</td>\n",
       "      <td>6.343282</td>\n",
       "      <td>6.995584</td>\n",
       "      <td>6.545068</td>\n",
       "      <td>5.778267</td>\n",
       "      <td>5.364867</td>\n",
       "      <td>5.176726</td>\n",
       "      <td>3.736625</td>\n",
       "      <td>3.483445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.349193</td>\n",
       "      <td>7.703877</td>\n",
       "      <td>4.773705</td>\n",
       "      <td>5.888851</td>\n",
       "      <td>7.190732</td>\n",
       "      <td>6.201897</td>\n",
       "      <td>5.458743</td>\n",
       "      <td>2.115626</td>\n",
       "      <td>4.403845</td>\n",
       "      <td>5.072792</td>\n",
       "      <td>...</td>\n",
       "      <td>5.075056</td>\n",
       "      <td>4.688004</td>\n",
       "      <td>5.436282</td>\n",
       "      <td>7.887780</td>\n",
       "      <td>6.099140</td>\n",
       "      <td>4.711411</td>\n",
       "      <td>4.281257</td>\n",
       "      <td>4.531069</td>\n",
       "      <td>2.765506</td>\n",
       "      <td>2.996892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.788271</td>\n",
       "      <td>4.277224</td>\n",
       "      <td>3.017914</td>\n",
       "      <td>4.918998</td>\n",
       "      <td>5.275575</td>\n",
       "      <td>4.521624</td>\n",
       "      <td>4.595800</td>\n",
       "      <td>1.543686</td>\n",
       "      <td>3.181172</td>\n",
       "      <td>2.908593</td>\n",
       "      <td>...</td>\n",
       "      <td>5.570172</td>\n",
       "      <td>5.027438</td>\n",
       "      <td>3.377301</td>\n",
       "      <td>4.599360</td>\n",
       "      <td>5.143477</td>\n",
       "      <td>5.397762</td>\n",
       "      <td>2.934463</td>\n",
       "      <td>2.670879</td>\n",
       "      <td>2.712019</td>\n",
       "      <td>1.840912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.728169</td>\n",
       "      <td>4.428463</td>\n",
       "      <td>3.020306</td>\n",
       "      <td>3.091806</td>\n",
       "      <td>4.601205</td>\n",
       "      <td>3.741062</td>\n",
       "      <td>3.754669</td>\n",
       "      <td>1.743823</td>\n",
       "      <td>2.999665</td>\n",
       "      <td>2.155294</td>\n",
       "      <td>...</td>\n",
       "      <td>3.619874</td>\n",
       "      <td>3.033892</td>\n",
       "      <td>4.170179</td>\n",
       "      <td>4.387602</td>\n",
       "      <td>4.281016</td>\n",
       "      <td>3.418801</td>\n",
       "      <td>3.579430</td>\n",
       "      <td>3.362475</td>\n",
       "      <td>1.980857</td>\n",
       "      <td>1.903859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.485831</td>\n",
       "      <td>2.491964</td>\n",
       "      <td>3.123583</td>\n",
       "      <td>1.220706</td>\n",
       "      <td>2.476729</td>\n",
       "      <td>1.459165</td>\n",
       "      <td>1.287098</td>\n",
       "      <td>0.635384</td>\n",
       "      <td>1.591328</td>\n",
       "      <td>1.235657</td>\n",
       "      <td>...</td>\n",
       "      <td>2.119257</td>\n",
       "      <td>1.296225</td>\n",
       "      <td>2.666702</td>\n",
       "      <td>2.148987</td>\n",
       "      <td>2.304873</td>\n",
       "      <td>1.767338</td>\n",
       "      <td>2.090425</td>\n",
       "      <td>1.938429</td>\n",
       "      <td>1.406396</td>\n",
       "      <td>0.533590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Personal      Work   Studies   Meeting    School  Bussiness      News  \\\n",
       "0  5.922018  5.867961  5.000954  5.490351  6.148301   5.441613  5.914790   \n",
       "1  3.832694  3.845042  2.612650  2.576775  3.720851   3.291850  2.880136   \n",
       "2  6.069281  5.746724  5.241623  4.503539  6.150415   5.351885  4.313432   \n",
       "3  7.604465  9.902568  5.151183  6.706818  7.189224   7.277782  6.256900   \n",
       "4  6.349193  7.703877  4.773705  5.888851  7.190732   6.201897  5.458743   \n",
       "5  4.788271  4.277224  3.017914  4.918998  5.275575   4.521624  4.595800   \n",
       "6  3.728169  4.428463  3.020306  3.091806  4.601205   3.741062  3.754669   \n",
       "7  2.485831  2.491964  3.123583  1.220706  2.476729   1.459165  1.287098   \n",
       "\n",
       "   Subscriptions  Marketing   Clients  ...   Economy   Finance     Music  \\\n",
       "0       1.879558   3.822869  4.235974  ...  5.023850  5.183203  3.779370   \n",
       "1       1.034950   2.567267  2.384122  ...  3.213712  2.733543  4.429894   \n",
       "2       2.876343   4.056960  4.942735  ...  4.210724  4.839167  3.281057   \n",
       "3       2.989461   5.743333  6.160741  ...  6.047608  5.063769  6.343282   \n",
       "4       2.115626   4.403845  5.072792  ...  5.075056  4.688004  5.436282   \n",
       "5       1.543686   3.181172  2.908593  ...  5.570172  5.027438  3.377301   \n",
       "6       1.743823   2.999665  2.155294  ...  3.619874  3.033892  4.170179   \n",
       "7       0.635384   1.591328  1.235657  ...  2.119257  1.296225  2.666702   \n",
       "\n",
       "     Family   College     Bills     Games    Design  Advertisements   Reviews  \n",
       "0  5.361845  5.124218  5.183096  3.625114  3.316906        2.809899  2.550122  \n",
       "1  3.707040  3.608046  2.913475  2.674548  3.219880        1.654688  1.225528  \n",
       "2  5.602248  5.394762  5.273412  2.706662  3.450992        2.888514  2.491245  \n",
       "3  6.995584  6.545068  5.778267  5.364867  5.176726        3.736625  3.483445  \n",
       "4  7.887780  6.099140  4.711411  4.281257  4.531069        2.765506  2.996892  \n",
       "5  4.599360  5.143477  5.397762  2.934463  2.670879        2.712019  1.840912  \n",
       "6  4.387602  4.281016  3.418801  3.579430  3.362475        1.980857  1.903859  \n",
       "7  2.148987  2.304873  1.767338  2.090425  1.938429        1.406396  0.533590  \n",
       "\n",
       "[8 rows x 36 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Here we firstly have a list of categories one of which will be assigned a category\n",
    "##Here we calculate similarity of each of the top twenty words of a topic with each category, \n",
    "##sum up similarity index for each category for that topic and assign it with the highest one\n",
    "\n",
    "\n",
    "spacy_model =spacy.load('en_core_web_md')\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(spacy_model.vocab)\n",
    "#list_of_categories= ['Personal', 'Professional', 'Social','Sports','Study','Health', 'Politics', 'Business', 'Science','Music']\n",
    "list_of_categories =['Personal','Work' ,'Studies', 'Meeting', 'School', 'Bussiness', 'News', 'Subscriptions', 'Marketing', 'Clients',\n",
    "                     'Sports', 'Extracurricular', 'Health', 'Travel', 'Schedule', 'Photography',\n",
    "                    'Politics', 'Cooking', 'Fashion', 'Social', 'Fitness', 'Research', 'Science', 'Technology', 'Sales',\n",
    "                    'Shopping', 'Economy', 'Finance', 'Music', 'Family', 'College', 'Bills', 'Games', 'Design', 'Advertisements',\n",
    "                    'Reviews']\n",
    "string_cat=' '.join([str(elem) for elem in list_of_categories])\n",
    "categories = tokenizer(string_cat)\n",
    "\n",
    "df = pd.DataFrame(0,index=topic_num_list,columns=list_of_categories)\n",
    "\n",
    "for category in categories:\n",
    "    for key, topic_words in topic_dict.items():\n",
    "        df.loc[key,category.text] = calculate_similarity(topic_words, category)\n",
    "        #print(key, category.text)\n",
    "\n",
    "\n",
    "for value in topic_dict.values():\n",
    "    \n",
    "    print(value)\n",
    "    print('\\n')\n",
    "\n",
    "df\n",
    "\n",
    "######################################### Things that are left to do:  ################### \n",
    "\n",
    "#Run model iteratively until corellation of each topic with a category is above 8 (or has run 10 times)\n",
    "#Run model iteratively until all topics have a seperate category (or has run 10 times)\n",
    "#Run model iteratively until we get the best coherence \n",
    "\n",
    "\n",
    "\n",
    "#There need to be rules on how much a topic's correlation with a category needs to be for it to be put into it \n",
    "\n",
    "#We can do something like after clustering if even one of the topics has similarity with every category less thatn 8 we repeat the \n",
    "#clustering and we repeat it till all topics have similarity with one of the categories\n",
    "\n",
    "#What happens when two topics highly corelated with a single category, that will probably not happen if the list of categories is large\n",
    "\n",
    "#Try doing that thing where they keep repeating LDA until it has a certain coherence score (you have the saved in OneTab I think)\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e0c18e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.554836190654896\n",
      "{0: 'School', 1: 'Unmarked', 2: 'Health', 3: 'Work', 4: 'Family', 5: 'Politics', 6: 'Unmarked', 7: 'Unmarked'}\n",
      "                                               Article  Topic  Category\n",
      "0    Older women who look on the bright side of lif...     12       NaN\n",
      "1    In Bangladesh, a new report finds, impoverishe...      3      Work\n",
      "2    When he first moved to Miami, Waltter Teruel s...     12       NaN\n",
      "3    When ATT, a leading Internet provider, propose...      3      Work\n",
      "4      Donald Trump is on a tour of battleground st...      3      Work\n",
      "..                                                 ...    ...       ...\n",
      "504  The number of law enforcement officers shot an...      0    School\n",
      "505    Trump is busy these days with victory tours,...      3      Work\n",
      "506  It’s always interesting for the Goats and Soda...      4    Family\n",
      "507  The election of Donald Trump was a surprise to...      5  Politics\n",
      "508  Voters in the English city of Sunderland did s...      4    Family\n",
      "\n",
      "[509 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "topic_category_dict=df.idxmax(axis=1).to_dict() ##dictionary with all topics with their corresponding categories\n",
    "\n",
    "topic_category_correlation_dict=df.max(axis=1).to_dict() ##dictionary with all topics with their correlation to their categories\n",
    "\n",
    "thresh=sum(topic_category_correlation_dict.values())/len(topic_category_correlation_dict)   \n",
    "\n",
    "print(thresh)\n",
    "\n",
    "\n",
    "for key, value in topic_category_correlation_dict.items():\n",
    "    if topic_category_correlation_dict.get(key)<math.floor(thresh):\n",
    "        topic_category_dict[key]='Unmarked'\n",
    "\n",
    "print(topic_category_dict)\n",
    "\n",
    "npr['Category']= npr['Topic'].map(topic_category_dict)\n",
    "print(npr)\n",
    "\n",
    "npr['Category'].value_counts(dropna=False)\n",
    "\n",
    "npr_remaining=npr[npr['Category']=='Unmarked']\n",
    "\n",
    "npr_remaining.to_csv('npr_remaining.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f9599d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#list_categories=list(topic_category_dict.values())\n",
    "\n",
    "##Firstly lets check for any duplicates within the topic's categories and reassign these to next best option\n",
    "#run loop on dictionary with names of category \n",
    "#find similar ones, compare their correlations\n",
    "#let the higher correlated one keep its category \n",
    "\n",
    "#all_distinct_categories= False\n",
    "\n",
    "#rev_multidict = {}\n",
    "#for key, value in topic_category_dict.items():\n",
    "#    rev_multidict.setdefault(value, set()).add(key) \n",
    "\n",
    "#rev_multidict.update((k,list(v)) for k,v in rev_multidict.items()) \n",
    "\n",
    "#rev_multidict2= {}\n",
    "#for key,value in rev_multidict.items(): rev_multidict2[key] = [topic_category_correlation_dict.get(item) for item in value]\n",
    "\n",
    "#print(rev_multidict2)\n",
    "#{k: [item: topic_category_correlation_dict.get(item) for item in v ] for k, v in rev_multidict2.items()}    \n",
    "    \n",
    "#while !all_distinct_categories:\n",
    "#if :\n",
    "#    all_distinct_categories=True    \n",
    "#else:\n",
    " #   all_distinct_categories=False\n",
    "  #  rev_multidict = {}\n",
    "   # for key, value in topic_category_dict.items():\n",
    "    #    rev_multidict.setdefault(value, set()).add(key) \n",
    "\n",
    "    #repeated_values=[values for key, values in rev_multidict.items() if len(values) > 1]\n",
    "    #for set1 in repeated_values:\n",
    "     #   temp_list= []\n",
    "      #  for value in set1:\n",
    "       #     temp_list.append(topic_category_correlation_dict.get(value))\n",
    "\n",
    "#for key, value in rev_multidict:\n",
    " #   for topic in value:\n",
    "  #      if topic_category_correlation_dict.get(topic)\n",
    "        \n",
    "   #     dfnlargest(2).values[-1]\n",
    "    #    if df.get_value(topic, key)  \n",
    "            \n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "#print(rev_multidict)\n",
    "\n",
    "\n",
    "\n",
    "##Next we can check whether the correlation is above a certain number,\n",
    "#if it is we keep the cateogry otherwise those emails will be unmarked\n",
    "\n",
    "\n",
    "\n",
    "#print(df.idxmax(axis=1))\n",
    "#print(this_dictionary)\n",
    "#print(df.max(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3eec465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -8.453472005712328\n",
      "\n",
      "Coherence Score:  0.37687023913139384\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a988581",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98428459",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from itertools import chain\n",
    "#tokenized_list = list(chain(*data_lemmatized))\n",
    "\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "#cv =CountVectorizer(max_df=0.9, min_df=2, stop_words='english')\n",
    "#dtm = cv.fit_transform(tokenized_list)\n",
    "\n",
    "#corpus = gensim.matutils.Sparse2Corpus(dtm, documents_columns=False)\n",
    "#id_map = dict((v, k) for k, v in cv.vocabulary_.items())\n",
    "\n",
    "#lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           #id2word=id_map,\n",
    "                                           #num_topics=5, \n",
    "                                           #random_state=110,\n",
    "                                           #update_every=1,\n",
    "                                           #chunksize=100,\n",
    "                                           #passes=10,\n",
    "                                           #alpha='auto',\n",
    "                                           #per_word_topics=True)\n",
    "#lda_corpus = lda_model[corpus]\n",
    "#from pprint import pprint\n",
    "#pprint(lda_model.print_topics())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecc49ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#what I should still be doing is comparing the highest probability that a document belongs to a topic\n",
    "#with some kind of a threshold. If the document's probability is below this threshold it will be categorized with no category\n",
    "\n",
    "\n",
    "#from itertools import chain\n",
    "#lda_corpus = lda_model[corpus]\n",
    "#scores = list(chain(*[[score for topic_id,score in topic] \\\n",
    "                      #for topic in [doc for doc in lda_corpus]]))\n",
    "#scores = []\n",
    "#for doc in lda_corpus:\n",
    " #   for topic in doc:\n",
    "  #      for topic_id, score in topic:\n",
    "   #         scores.append(score)\n",
    "#threshold = sum(scores)/len(scores)\n",
    "#threshold = sum(scores)/len(scores)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}