{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "##Imports for gensim, Machine Learning LDA\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "##Imports for NLP\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ReportLab imports, Generating PDFs\n",
    "from reportlab.pdfgen import canvas\n",
    "import textwrap\n",
    "import os\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.pdfbase.pdfmetrics import stringWidth\n",
    "import errno\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# NLTK Stop words\n",
    "#stop_words = stopwords.words('english')\n",
    "\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_dataset = pd.read_excel('custom_dataset.xlsx') \n",
    "email_dataset_required=email_dataset.drop(['Thread ID', 'Message ID', 'Reply-to','URL Links', 'RFC 822 Message ID', 'Email in PDF', 'Email Addresses'], axis = 1)\n",
    "email_dataset_bodies=email_dataset_required.drop(['From', 'To', 'Cc','Bcc', 'Subject', 'Date & Time Sent', 'Date & Time Received', 'Labels'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Email Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Here are this week's five links that are worth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is my final \"links worth your time\" list ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Here are this week's five links that are worth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hey there!\\nWe are now 10 full days into the n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Here are this week's five links that are worth...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Email Text\n",
       "0  Here are this week's five links that are worth...\n",
       "1  This is my final \"links worth your time\" list ...\n",
       "2  Here are this week's five links that are worth...\n",
       "3  Hey there!\\nWe are now 10 full days into the n...\n",
       "4  Here are this week's five links that are worth..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_dataset_bodies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "data = email_dataset_bodies.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\",disable=['parser']) \n",
    " \n",
    "##Adding additional stopwords\n",
    "new_stopwords= ['dear', 'thanks','regards', 'hello','hi', 'bye','goodbye', 'say']\n",
    "for word in new_stopwords:\n",
    "    nlp.Defaults.stop_words.add(word) \n",
    "    nlp.vocab[word].is_stop = True\n",
    "    \n",
    "def remove_stopwords_spacy(texts):\n",
    "    return [[word.text for word in nlp(str(text)) if not word.is_stop] for text in texts]\n",
    "\n",
    "def remove_stopwords_gensim(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def remove_non_vocab(texts):\n",
    "    return [[word for word in doc if word in words or not word.isalpha()] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords_spacy(data_words)\n",
    "\n",
    "#remove words that only occur once to make process faster\n",
    "#all_tokens = sum(data_words_nostops, [])\n",
    "#tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)\n",
    "#text_no_single_words = [[term for term in words if term not in tokens_once] for words in data_words_nostops]\n",
    "\n",
    "data_vocab_words=remove_non_vocab(data_words_nostops)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_vocab_words)\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus \n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score [0.6363873540754271, 0.5447441159985058, 0.6046194799352524, 0.6364152430394319, 0.46695628900238556, 0.4548813370249462, 0.47862189403385125, 0.5313816494982265, 0.5252433225926189, 0.4965254003381303, 0.4729640101794628, 0.5113232496100382, 0.4613466330239862, 0.5836472878628458, 0.4999093335829733, 0.4792580806961465, 0.4565894422491537, 0.537124199990942, 0.5187633387122961, 0.4765634476653946]\n",
      "Perplexity Score [-6.391843051554629, -6.339346704094267, -6.344207959469708, -6.3585692348757625, -6.381436030334284, -6.3985415911848715, -6.4144543041366155, -6.415406328372459, -6.433452019336887, -6.433843302085483, -6.463558433320721, -6.4561269695300805, -6.46463870213524, -6.52546596468036, -6.6044618732497025, -6.642596133052682, -6.658619546619261, -6.691374169042024, -6.691466586736132, -6.732172423441828]\n"
     ]
    }
   ],
   "source": [
    "# Build LDA model\n",
    "coherence_score=[]\n",
    "perplexity_score=[]\n",
    "for i in range(1,21):\n",
    "    \n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                               id2word=id2word,\n",
    "                                               num_topics=i, \n",
    "                                               random_state=110,\n",
    "                                               update_every=1,\n",
    "                                               chunksize=100,\n",
    "                                               passes=10,\n",
    "                                               alpha='auto',\n",
    "                                               per_word_topics=False,\n",
    "                                               minimum_probability=0.0)\n",
    "\n",
    "    perplexity_score.append(lda_model.log_perplexity(corpus))\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    coherence_score.append(coherence_model_lda.get_coherence())\n",
    "\n",
    "print('Coherence Score', coherence_score)\n",
    "\n",
    "print('Perplexity Score',perplexity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = lambda i: coherence_score[i]\n",
    "\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                               id2word=id2word,\n",
    "                                               num_topics=max(range(len(coherence_score)), key=f)+1, \n",
    "                                               random_state=110,\n",
    "                                               update_every=1,\n",
    "                                               chunksize=100,\n",
    "                                               passes=10,\n",
    "                                               alpha='auto',\n",
    "                                               per_word_topics=False,\n",
    "                                               minimum_probability=0.0)\n",
    "\n",
    "\n",
    "max(range(len(coherence_score)), key=f)+1\n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marking each email with topic\n",
    "from itertools import chain\n",
    "lda_corpus = lda_model[corpus]\n",
    "\n",
    "all_topics=lda_model.get_document_topics(corpus)\n",
    "\n",
    "scores = list(chain(*[[score for topic_id,score in topic] \\\n",
    "                      for topic in [doc for doc in lda_corpus]]))\n",
    "threshold = sum(scores)/len(scores)\n",
    "\n",
    "\n",
    "all_topics_csr = gensim.matutils.corpus2csc(all_topics)\n",
    "all_topics_numpy = all_topics_csr.T.toarray()\n",
    "categorized_docs=[]\n",
    "\n",
    "for document in all_topics_numpy:\n",
    "    if max(document) > threshold:\n",
    "        categorized_docs.append(document.argmax())\n",
    "    else:\n",
    "        categorized_docs.append('Unmarked')\n",
    "\n",
    "email_dataset_bodies = email_dataset_bodies.assign(Topic = categorized_docs)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    106\n",
       "2      8\n",
       "Name: Topic, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_dataset_bodies['Topic'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Taking top 20 words\n",
    "topics_with_words=[] #list that contains words belonging to each topic\n",
    "topic_num_list=[] #topic numbers\n",
    "for i in range(max(range(len(coherence_score)), key=f)+1):\n",
    "    tt = lda_model.get_topic_terms(i,20)\n",
    "    topics_with_words.append([id2word[pair[0]] for pair in tt]) #filling list with each topics word\n",
    "    topic_num_list.append(i) #filling up the topic number list   \n",
    "    \n",
    "#removing stopwords again and removing words that are not in the vocab     \n",
    "for topic in topics_with_words:\n",
    "    topic= [word for word in topic if nlp.vocab.has_vector(word)]\n",
    "    topic= [word for word in topic if not nlp.vocab[word].is_stop]     \n",
    "\n",
    "topic_dict = dict(zip(topic_num_list, topics_with_words))#buliding a dictionary with list of topic numbers and list of words in those topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that takes in topic words and a category and returns similary between the two\n",
    "#topic is a list and category should be passsed in as a spacy token\n",
    "def calculate_similarity(topic_words, category):\n",
    "    total_sim=0\n",
    "    string=''\n",
    "    string = ' '.join([str(elem) for elem in topic_words])## this is the string that contains every word in that topic\n",
    "    doc= spacy_model(string) ##topic's string is used created a spacy doc\n",
    "    for token in doc:\n",
    "        total_sim+= token.similarity(category)\n",
    "    return total_sim    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Computer</th>\n",
       "      <th>Programming</th>\n",
       "      <th>Databases</th>\n",
       "      <th>Software</th>\n",
       "      <th>Web</th>\n",
       "      <th>Interviews</th>\n",
       "      <th>Productivity</th>\n",
       "      <th>Games</th>\n",
       "      <th>Coding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.712211</td>\n",
       "      <td>4.072159</td>\n",
       "      <td>2.955722</td>\n",
       "      <td>3.869267</td>\n",
       "      <td>4.745250</td>\n",
       "      <td>3.448182</td>\n",
       "      <td>3.541660</td>\n",
       "      <td>3.928754</td>\n",
       "      <td>3.662122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.269766</td>\n",
       "      <td>5.665208</td>\n",
       "      <td>4.166120</td>\n",
       "      <td>5.592243</td>\n",
       "      <td>7.146331</td>\n",
       "      <td>4.427041</td>\n",
       "      <td>3.837623</td>\n",
       "      <td>4.871092</td>\n",
       "      <td>4.811071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.765348</td>\n",
       "      <td>4.815424</td>\n",
       "      <td>3.693732</td>\n",
       "      <td>5.223631</td>\n",
       "      <td>6.267658</td>\n",
       "      <td>4.507785</td>\n",
       "      <td>4.136917</td>\n",
       "      <td>5.574654</td>\n",
       "      <td>4.011449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.192877</td>\n",
       "      <td>3.264017</td>\n",
       "      <td>1.907840</td>\n",
       "      <td>3.194795</td>\n",
       "      <td>3.724812</td>\n",
       "      <td>2.868111</td>\n",
       "      <td>2.936002</td>\n",
       "      <td>3.547936</td>\n",
       "      <td>2.669207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Computer  Programming  Databases  Software       Web  Interviews  \\\n",
       "0  4.712211     4.072159   2.955722  3.869267  4.745250    3.448182   \n",
       "1  6.269766     5.665208   4.166120  5.592243  7.146331    4.427041   \n",
       "2  6.765348     4.815424   3.693732  5.223631  6.267658    4.507785   \n",
       "3  4.192877     3.264017   1.907840  3.194795  3.724812    2.868111   \n",
       "\n",
       "   Productivity     Games    Coding  \n",
       "0      3.541660  3.928754  3.662122  \n",
       "1      3.837623  4.871092  4.811071  \n",
       "2      4.136917  5.574654  4.011449  \n",
       "3      2.936002  3.547936  2.669207  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Here we firstly have a list of categories one of which will be assigned a category\n",
    "##Here we calculate similarity of each of the top twenty words of a topic with each category, \n",
    "##sum up similarity index for each category for that topic and assign it with the highest one\n",
    "\n",
    "spacy_model =spacy.load('en_core_web_md')\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(spacy_model.vocab)\n",
    "#list_of_categories =['Personal','Work' ,'Studies', 'Meeting', 'School', 'Bussiness', 'News', 'Subscriptions', 'Marketing', 'Clients',\n",
    "#                     'Sports', 'Extracurricular', 'Health', 'Travel', 'Schedule', 'Photography',\n",
    "#                     'Politics', 'Cooking', 'Fashion', 'Social', 'Fitness', 'Research', 'Science', 'Technology', 'Sales',\n",
    "#                     'Shopping', 'Economy', 'Finance', 'Music', 'Family', 'College', 'Bills', 'Games', 'Design', 'Advertisements',\n",
    "#                     'Reviews']\n",
    "#Computer-Science\n",
    "#Programming-Languages\n",
    "#Databases\n",
    "#Cloud computing\n",
    "#Web-Design\n",
    "#Software-Development\n",
    "#Machine-Learning\n",
    "#Coding-Interviews\n",
    "#Games\n",
    "#Productivity\n",
    "list_of_categories= ['Computer','Programming','Databases','Software','Web','Interviews','Productivity','Games', 'Coding' ]\n",
    "\n",
    "string_cat=' '.join([str(elem) for elem in list_of_categories])\n",
    "categories = tokenizer(string_cat)\n",
    "\n",
    "category_similarity_df = pd.DataFrame(0,index=topic_num_list,columns=list_of_categories)\n",
    "\n",
    "for category in categories:\n",
    "    for key, topic_words in topic_dict.items():\n",
    "        category_similarity_df.loc[key,category.text] = calculate_similarity(topic_words, category)\n",
    "\n",
    "category_similarity_df\n",
    "\n",
    "######################################### Things that are left to do:  ################### \n",
    "\n",
    "#Run model iteratively until corellation of each topic with a category is above 8 (or has run 10 times)\n",
    "#Run model iteratively until all topics have a seperate category (or has run 10 times)\n",
    "#Run model iteratively until we get the best coherence \n",
    "\n",
    "\n",
    "\n",
    "#There need to be rules on how much a topic's correlation with a category needs to be for it to be put into it \n",
    "\n",
    "#We can do something like after clustering if even one of the topics has similarity with every category less thatn 8 we repeat the \n",
    "#clustering and we repeat it till all topics have similarity with one of the categories\n",
    "\n",
    "#What happens when two topics highly corelated with a single category, that will probably not happen if the list of categories is large\n",
    "\n",
    "#Try doing that thing where they keep repeating LDA until it has a certain coherence score (you have the saved in OneTab I think)\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Unmarked', 1: 'Web', 2: 'Computer', 3: 'Unmarked'}\n",
      "Web         106\n",
      "Computer      8\n",
      "Name: Labels, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "topic_category_dict=category_similarity_df.idxmax(axis=1).to_dict() ##dictionary with all topics with their corresponding categories\n",
    "\n",
    "topic_category_correlation_dict=category_similarity_df.max(axis=1).to_dict() ##dictionary with all topics with their correlation to their categories\n",
    "\n",
    "thresh=sum(topic_category_correlation_dict.values())/len(topic_category_correlation_dict)   \n",
    "\n",
    "for key, value in topic_category_correlation_dict.items():\n",
    "    if topic_category_correlation_dict.get(key)<math.floor(thresh):\n",
    "        topic_category_dict[key]='Unmarked'\n",
    "\n",
    "print(topic_category_dict)\n",
    "\n",
    "email_dataset_bodies['Labels']= email_dataset_bodies['Topic'].map(topic_category_dict)\n",
    "\n",
    "email_dataset_bodies\n",
    "\n",
    "print(email_dataset_bodies['Labels'].value_counts(dropna=False))\n",
    "\n",
    "email_dataset_bodies_remaining=email_dataset_bodies[email_dataset_bodies['Labels']=='Unmarked']\n",
    "email_dataset_required['Labels'] = email_dataset_bodies['Labels']\n",
    "\n",
    "#email_dataset_required = email_dataset_required.merge(email_dataset_bodies,on='Labels',how=\"left\")\n",
    "#email_dataset_bodies_remaining.to_csv('email_dataset_bodies_remaining.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -6.358569226120446\n",
      "\n",
      "Coherence Score:  0.6364152430394319\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>From</th>\n",
       "      <th>To</th>\n",
       "      <th>Cc</th>\n",
       "      <th>Bcc</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Date &amp; Time Sent</th>\n",
       "      <th>Date &amp; Time Received</th>\n",
       "      <th>Email Text</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>quincy@freecodecamp.org</td>\n",
       "      <td>malvisbid@gmail.com</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Learn JavaScript - our free 134-part video cou...</td>\n",
       "      <td>2018-12-13 15:25:58</td>\n",
       "      <td>2021-04-21 16:09:46</td>\n",
       "      <td>Here are this week's five links that are worth...</td>\n",
       "      <td>Web</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>quincy@freecodecamp.org</td>\n",
       "      <td>malvisbid@gmail.com</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Learn React.js - a free 5-hour course for begi...</td>\n",
       "      <td>2018-12-21 01:09:04</td>\n",
       "      <td>2021-04-21 16:09:44</td>\n",
       "      <td>This is my final \"links worth your time\" list ...</td>\n",
       "      <td>Web</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>quincy@freecodecamp.org</td>\n",
       "      <td>malvisbid@gmail.com</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>The React Handbook - a massive free guide to b...</td>\n",
       "      <td>2019-01-10 20:19:18</td>\n",
       "      <td>2021-04-21 16:37:51</td>\n",
       "      <td>Here are this week's five links that are worth...</td>\n",
       "      <td>Web</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thomas@collegeinfogeek.com</td>\n",
       "      <td>malvisbid@gmail.com</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Build a complete productivity system - my firs...</td>\n",
       "      <td>2019-01-10 20:56:21</td>\n",
       "      <td>2021-04-21 16:09:42</td>\n",
       "      <td>Hey there!\\nWe are now 10 full days into the n...</td>\n",
       "      <td>Computer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>quincy@freecodecamp.org</td>\n",
       "      <td>malvisbid@gmail.com</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>How to build your own e-commerce website from ...</td>\n",
       "      <td>2019-01-17 20:45:13</td>\n",
       "      <td>2021-04-21 16:09:41</td>\n",
       "      <td>Here are this week's five links that are worth...</td>\n",
       "      <td>Web</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         From                   To Cc Bcc  \\\n",
       "0     quincy@freecodecamp.org  malvisbid@gmail.com          \n",
       "1     quincy@freecodecamp.org  malvisbid@gmail.com          \n",
       "2     quincy@freecodecamp.org  malvisbid@gmail.com          \n",
       "3  thomas@collegeinfogeek.com  malvisbid@gmail.com          \n",
       "4     quincy@freecodecamp.org  malvisbid@gmail.com          \n",
       "\n",
       "                                             Subject    Date & Time Sent  \\\n",
       "0  Learn JavaScript - our free 134-part video cou... 2018-12-13 15:25:58   \n",
       "1  Learn React.js - a free 5-hour course for begi... 2018-12-21 01:09:04   \n",
       "2  The React Handbook - a massive free guide to b... 2019-01-10 20:19:18   \n",
       "3  Build a complete productivity system - my firs... 2019-01-10 20:56:21   \n",
       "4  How to build your own e-commerce website from ... 2019-01-17 20:45:13   \n",
       "\n",
       "  Date & Time Received                                         Email Text  \\\n",
       "0  2021-04-21 16:09:46  Here are this week's five links that are worth...   \n",
       "1  2021-04-21 16:09:44  This is my final \"links worth your time\" list ...   \n",
       "2  2021-04-21 16:37:51  Here are this week's five links that are worth...   \n",
       "3  2021-04-21 16:09:42  Hey there!\\nWe are now 10 full days into the n...   \n",
       "4  2021-04-21 16:09:41  Here are this week's five links that are worth...   \n",
       "\n",
       "     Labels  \n",
       "0       Web  \n",
       "1       Web  \n",
       "2       Web  \n",
       "3  Computer  \n",
       "4       Web  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_dataset_required.fillna('', inplace=True)\n",
    "\n",
    "grouped = email_dataset_required.groupby('Labels')\n",
    "email_dataset_required.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def createPdf(efrom, to, cc, bcc, subject, date_sent, date_recieved, body, canvas):\n",
    "    \n",
    "    starting_y=800\n",
    "    offset_y=0\n",
    "    #pdf_file = subject\n",
    "    #outfilepath = os.path.join( path, pdf_file )\n",
    "    \n",
    "    font_size= 11\n",
    "    font_type_titles='Courier-Bold'\n",
    "    font_type_normal= 'Courier'\n",
    "    \n",
    "    \n",
    "    canvas.setFont(font_type_titles, font_size)\n",
    "    canvas.drawString(30,starting_y,'Subject:')\n",
    "    canvas.setFont(font_type_normal, font_size)\n",
    "    canvas.drawString(stringWidth('Subject:', font_type_titles, font_size) +35,starting_y, subject)\n",
    "    offset_y+=15\n",
    "\n",
    "    canvas.setFont(font_type_titles, font_size)\n",
    "    canvas.drawString(30,starting_y-offset_y,'From:')\n",
    "    canvas.setFont(font_type_normal, font_size)\n",
    "    canvas.drawString(stringWidth('From:', font_type_titles, font_size) +35,starting_y-offset_y,efrom)\n",
    "    offset_y+=15\n",
    "\n",
    "    canvas.setFont(font_type_titles, font_size)\n",
    "    canvas.drawString(30,starting_y-offset_y,'CC:')\n",
    "    canvas.setFont(font_type_normal, font_size)\n",
    "    canvas.drawString( stringWidth('CC:', font_type_titles, font_size) +35,starting_y-offset_y,cc)\n",
    "    offset_y+=15\n",
    "\n",
    "    canvas.setFont(font_type_titles, font_size)\n",
    "    canvas.drawString(30,starting_y-offset_y,'BCC:')\n",
    "    canvas.setFont(font_type_normal, font_size)\n",
    "    canvas.drawString(stringWidth('BCC:', font_type_titles, font_size) +35,starting_y-offset_y,bcc)\n",
    "    offset_y+=15\n",
    "\n",
    "    canvas.setFont(font_type_titles, font_size)\n",
    "    canvas.drawString(30,starting_y-offset_y,'To:')\n",
    "    canvas.setFont(font_type_normal, font_size)\n",
    "    canvas.drawString(stringWidth('To:', font_type_titles, font_size) +35,starting_y-offset_y,to)\n",
    "    offset_y+=15\n",
    "\n",
    "    canvas.setFont(font_type_titles, font_size)\n",
    "    canvas.drawString(30,starting_y-offset_y,'Date Sent:')\n",
    "    canvas.setFont(font_type_normal, font_size)\n",
    "    canvas.drawString(stringWidth('Data Sent:', font_type_titles, font_size) +35,starting_y-offset_y,date_sent)\n",
    "    offset_y+=15\n",
    "\n",
    "    canvas.setFont(font_type_titles, font_size)\n",
    "    canvas.drawString(30,starting_y-offset_y,'Date Recieved:')\n",
    "    canvas.setFont(font_type_normal, font_size)\n",
    "    canvas.drawString(stringWidth('Date Recieved:', font_type_titles, font_size) +35,starting_y-offset_y,date_recieved)\n",
    "    offset_y+=15\n",
    "\n",
    "    canvas.setLineWidth(.7)\n",
    "    canvas.line(30,700,580,700)\n",
    "    \n",
    "    canvas.setFont(font_type_normal, 10.5)\n",
    "    coory=690-15\n",
    "    if len(body) > 70:\n",
    "        wrap_text = textwrap.wrap(body, width=85)\n",
    "        for line in wrap_text:\n",
    "            canvas.drawString(30, coory, line)\n",
    "            coory-=15\n",
    "    else:\n",
    "        canvas.drawString(30, 690-15, body)\n",
    "    \n",
    "\n",
    "    canvas.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a directory\n",
    "path_to_create_directory='C:/Users/HP/Downloads'\n",
    "\n",
    "##before we start iterating over the dataframe, we have to group it according to Category\n",
    "grouped = email_dataset_required.groupby('Labels')\n",
    "\n",
    "\n",
    "for group_name, email_dataset_required_group in grouped:\n",
    "    path=''\n",
    "    new_dir= group_name\n",
    "    path = os.path.join(path_to_create_directory, new_dir)\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise\n",
    "           \n",
    "    columns_list=email_dataset_required_group.columns.to_list()\n",
    "    for i in range(len(email_dataset_required_group)):\n",
    "        createPdf(email_dataset_required_group.iloc[i, 0],email_dataset_required_group.iloc[i, 1],email_dataset_required_group.iloc[i, 2],email_dataset_required_group.iloc[i, 3],email_dataset_required_group.iloc[i, 4],\n",
    "                  email_dataset_required_group.iloc[i, 5].strftime(\"%H:%M:%S.%f - %b %d %Y\"),\n",
    "                  email_dataset_required_group.iloc[i, 6].strftime(\"%H:%M:%S.%f - %b %d %Y\") ,email_dataset_required_group.iloc[i, 7],\n",
    "                  canvas.Canvas(os.path.join(path, re.sub('[^A-Za-z0-9]+', '', email_dataset_required_group.iloc[i, 4])+\".pdf\")))\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#newString=''\n",
    "#data_words = list(sent_to_words(newString))\n",
    "#removeStopwords=\n",
    "#data_bigrammed=\n",
    "\n",
    "#new_doc=newString \n",
    "#new_doc_bow = dictionary.doc2bow(new_doc)\n",
    "#ldamodel.get_document_topics(new_doc_bow)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}